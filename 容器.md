在 PyTorch 中，容器（Containers）是用于组织和管理神经网络层的工具，它们主要解决了以下问题：

1. **层的结构化管理**：帮助我们以模块化和层次化的方式组织网络结构。
2. **模型代码简洁**：减少重复代码，使网络构建更加高效。
3. **灵活性与动态性**：支持复杂网络拓扑结构（如多分支、多模块的网络设计）。
4. **自动化处理**：能够自动将所有子模块的参数添加到 `model.parameters()` 中，方便优化。

---

## **常见容器及其作用**

PyTorch 提供了以下主要的容器：

### **1. `torch.nn.Sequential`**

- **作用**：按顺序将多个层连接起来，适合线性（顺序执行）的网络结构。
- **特点**：
    - 层按照定义的顺序依次执行。
    - 可通过下标索引访问各层。

#### **使用场景**

1. 构建简单的网络，例如多层感知机（MLP）。
2. 快速定义神经网络原型。

#### **示例**
```python
import torch.nn as nn # 定义一个简单的两层全连接网络 
model = nn.Sequential( nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5) ) 

# 查看模型结构 
print(model) 

# 输入一个张量 
input = torch.randn(3, 10) # 3个样本，每个样本有10个特征 
output = model(input) print(output.shape) # 输出形状为 (3, 5)
```

---

### **2. `torch.nn.ModuleList`**

- **作用**：管理一组 `nn.Module` 对象的有序列表。
- **特点**：
    - 不会自动定义 `forward` 方法，需显式编写前向传播逻辑。
    - 支持灵活的动态网络结构，允许在网络构建过程中动态修改或添加层。

#### **使用场景**

1. 动态定义层数（如循环创建层）。
2. 需要按自定义逻辑调用每一层。

#### **示例**
```python
# 使用 ModuleList 动态创建多层网络 
class DynamicNet(nn.Module): 
	def __init__(self): 
		super().__init__() 
		self.layers = nn.ModuleList([nn.Linear(10, 10) 
		for _ in range(5)]) # 创建5个线性层 
		self.relu = nn.ReLU() 

	def forward(self, x): 
		for layer in self.layers: # 动态调用每一层 
			x = self.relu(layer(x)) 
			return x 

model = DynamicNet() 
input = torch.randn(3, 10) 
output = model(input) 
print(output.shape) # 输出形状为 (3, 10)
```

---

### **3. `torch.nn.ModuleDict`**

- **作用**：以键值对的形式管理多个 `nn.Module` 对象。
- **特点**：
    - 支持通过名称（key）来访问和调用子模块。
    - 适合需要根据条件调用不同子模块的复杂网络。

#### **使用场景**

1. 需要灵活按名字调用子模块。
2. 需要动态定义不同分支（如多任务学习）。

#### **示例**
```python
# 使用 ModuleDict 管理不同分支
class MultiBranchNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.branches = nn.ModuleDict({
            'branch1': nn.Linear(10, 20),
            'branch2': nn.Linear(10, 30)
        })
        self.relu = nn.ReLU()

    def forward(self, x, branch_name):
        x = self.branches[branch_name](x)  # 按名字调用分支
        return self.relu(x)

model = MultiBranchNet()
input = torch.randn(3, 10)

# 调用不同的分支
output1 = model(input, 'branch1')
output2 = model(input, 'branch2')
print(output1.shape)  # 输出形状为 (3, 20)
print(output2.shape)  # 输出形状为 (3, 30)
```

---

### **4. `torch.nn.ParameterList`**

- **作用**：存储一组参数，适用于参数数量动态变化的场景。
- **特点**：
    - 用于管理可训练参数（`torch.nn.Parameter`）。
    - 不会自动定义前向传播逻辑。

#### **使用场景**

1. 动态定义网络参数（如多分支网络中每个分支的权重）。
2. 管理不属于固定层的参数（如注意力机制中的可学习参数）。

#### **示例**
```python
import torch
from torch.nn import Parameter

class CustomNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.params = nn.ParameterList([Parameter(torch.randn(10, 10)) for _ in range(5)])

    def forward(self, x):
        for param in self.params:
            x = x @ param  # 矩阵乘法
        return x

model = CustomNet()
input = torch.randn(3, 10)
output = model(input)
print(output.shape)  # 输出形状为 (3, 10)
```

---

### **5. `torch.nn.ParameterDict`**

- **作用**：以键值对形式管理一组参数。
- **特点**：
    - 参数按名称存储和调用。
    - 灵活性更高，适合多任务或模块化场景。

#### **使用场景**

1. 动态定义参数集合。
2. 多任务学习中，不同任务使用不同的参数。

#### **示例**
```python
class CustomNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.param_dict = nn.ParameterDict({
            'weight1': Parameter(torch.randn(10, 10)),
            'weight2': Parameter(torch.randn(10, 20))
        })

    def forward(self, x, param_name):
        return x @ self.param_dict[param_name]  # 按名称选择参数进行操作

model = CustomNet()
input = torch.randn(3, 10)
output = model(input, 'weight1')
print(output.shape)  # 输出形状为 (3, 10)
```

---
